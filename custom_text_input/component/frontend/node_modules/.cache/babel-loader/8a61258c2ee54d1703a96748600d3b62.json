{"ast":null,"code":"\"use strict\";\n\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\nvar _objectWithoutProperties = require(\"/home/morrissimons/Desktop/lysa-support-agent/custom_text_input/component/frontend/node_modules/@babel/runtime/helpers/objectWithoutProperties\");\nvar _objectSpread = require(\"/home/morrissimons/Desktop/lysa-support-agent/custom_text_input/component/frontend/node_modules/@babel/runtime/helpers/objectSpread2\");\nvar _taggedTemplateLiteral = require(\"/home/morrissimons/Desktop/lysa-support-agent/custom_text_input/component/frontend/node_modules/@babel/runtime/helpers/taggedTemplateLiteral\");\nconst _excluded = [\"vector_store_id\"];\nvar _templateObject, _templateObject2, _templateObject3, _templateObject4;\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.FileBatches = void 0;\nconst resource_1 = require(\"../../core/resource.js\");\nconst pagination_1 = require(\"../../core/pagination.js\");\nconst headers_1 = require(\"../../internal/headers.js\");\nconst sleep_1 = require(\"../../internal/utils/sleep.js\");\nconst Util_1 = require(\"../../lib/Util.js\");\nconst path_1 = require(\"../../internal/utils/path.js\");\nclass FileBatches extends resource_1.APIResource {\n  /**\n   * Create a vector store file batch.\n   */\n  create(vectorStoreID, body, options) {\n    return this._client.post((0, path_1.path)(_templateObject || (_templateObject = _taggedTemplateLiteral([\"/vector_stores/\", \"/file_batches\"])), vectorStoreID), _objectSpread(_objectSpread({\n      body\n    }, options), {}, {\n      headers: (0, headers_1.buildHeaders)([{\n        'OpenAI-Beta': 'assistants=v2'\n      }, options === null || options === void 0 ? void 0 : options.headers])\n    }));\n  }\n  /**\n   * Retrieves a vector store file batch.\n   */\n  retrieve(batchID, params, options) {\n    const {\n      vector_store_id\n    } = params;\n    return this._client.get((0, path_1.path)(_templateObject2 || (_templateObject2 = _taggedTemplateLiteral([\"/vector_stores/\", \"/file_batches/\", \"\"])), vector_store_id, batchID), _objectSpread(_objectSpread({}, options), {}, {\n      headers: (0, headers_1.buildHeaders)([{\n        'OpenAI-Beta': 'assistants=v2'\n      }, options === null || options === void 0 ? void 0 : options.headers])\n    }));\n  }\n  /**\n   * Cancel a vector store file batch. This attempts to cancel the processing of\n   * files in this batch as soon as possible.\n   */\n  cancel(batchID, params, options) {\n    const {\n      vector_store_id\n    } = params;\n    return this._client.post((0, path_1.path)(_templateObject3 || (_templateObject3 = _taggedTemplateLiteral([\"/vector_stores/\", \"/file_batches/\", \"/cancel\"])), vector_store_id, batchID), _objectSpread(_objectSpread({}, options), {}, {\n      headers: (0, headers_1.buildHeaders)([{\n        'OpenAI-Beta': 'assistants=v2'\n      }, options === null || options === void 0 ? void 0 : options.headers])\n    }));\n  }\n  /**\n   * Create a vector store batch and poll until all files have been processed.\n   */\n  async createAndPoll(vectorStoreId, body, options) {\n    const batch = await this.create(vectorStoreId, body);\n    return await this.poll(vectorStoreId, batch.id, options);\n  }\n  /**\n   * Returns a list of vector store files in a batch.\n   */\n  listFiles(batchID, params, options) {\n    const {\n        vector_store_id\n      } = params,\n      query = _objectWithoutProperties(params, _excluded);\n    return this._client.getAPIList((0, path_1.path)(_templateObject4 || (_templateObject4 = _taggedTemplateLiteral([\"/vector_stores/\", \"/file_batches/\", \"/files\"])), vector_store_id, batchID), pagination_1.CursorPage, _objectSpread(_objectSpread({\n      query\n    }, options), {}, {\n      headers: (0, headers_1.buildHeaders)([{\n        'OpenAI-Beta': 'assistants=v2'\n      }, options === null || options === void 0 ? void 0 : options.headers])\n    }));\n  }\n  /**\n   * Wait for the given file batch to be processed.\n   *\n   * Note: this will return even if one of the files failed to process, you need to\n   * check batch.file_counts.failed_count to handle this case.\n   */\n  async poll(vectorStoreID, batchID, options) {\n    var _options$pollInterval, _options$pollInterval2;\n    const headers = (0, headers_1.buildHeaders)([options === null || options === void 0 ? void 0 : options.headers, {\n      'X-Stainless-Poll-Helper': 'true',\n      'X-Stainless-Custom-Poll-Interval': (_options$pollInterval = options === null || options === void 0 ? void 0 : (_options$pollInterval2 = options.pollIntervalMs) === null || _options$pollInterval2 === void 0 ? void 0 : _options$pollInterval2.toString()) !== null && _options$pollInterval !== void 0 ? _options$pollInterval : undefined\n    }]);\n    while (true) {\n      const {\n        data: batch,\n        response\n      } = await this.retrieve(batchID, {\n        vector_store_id: vectorStoreID\n      }, _objectSpread(_objectSpread({}, options), {}, {\n        headers\n      })).withResponse();\n      switch (batch.status) {\n        case 'in_progress':\n          let sleepInterval = 5000;\n          if (options === null || options === void 0 ? void 0 : options.pollIntervalMs) {\n            sleepInterval = options.pollIntervalMs;\n          } else {\n            const headerInterval = response.headers.get('openai-poll-after-ms');\n            if (headerInterval) {\n              const headerIntervalMs = parseInt(headerInterval);\n              if (!isNaN(headerIntervalMs)) {\n                sleepInterval = headerIntervalMs;\n              }\n            }\n          }\n          await (0, sleep_1.sleep)(sleepInterval);\n          break;\n        case 'failed':\n        case 'cancelled':\n        case 'completed':\n          return batch;\n      }\n    }\n  }\n  /**\n   * Uploads the given files concurrently and then creates a vector store file batch.\n   *\n   * The concurrency limit is configurable using the `maxConcurrency` parameter.\n   */\n  async uploadAndPoll(vectorStoreId, _ref, options) {\n    var _options$maxConcurren;\n    let {\n      files,\n      fileIds = []\n    } = _ref;\n    if (files == null || files.length == 0) {\n      throw new Error(\"No `files` provided to process. If you've already uploaded files you should use `.createAndPoll()` instead\");\n    }\n    const configuredConcurrency = (_options$maxConcurren = options === null || options === void 0 ? void 0 : options.maxConcurrency) !== null && _options$maxConcurren !== void 0 ? _options$maxConcurren : 5;\n    // We cap the number of workers at the number of files (so we don't start any unnecessary workers)\n    const concurrencyLimit = Math.min(configuredConcurrency, files.length);\n    const client = this._client;\n    const fileIterator = files.values();\n    const allFileIds = [...fileIds];\n    // This code is based on this design. The libraries don't accommodate our environment limits.\n    // https://stackoverflow.com/questions/40639432/what-is-the-best-way-to-limit-concurrency-when-using-es6s-promise-all\n    async function processFiles(iterator) {\n      for (let item of iterator) {\n        const fileObj = await client.files.create({\n          file: item,\n          purpose: 'assistants'\n        }, options);\n        allFileIds.push(fileObj.id);\n      }\n    }\n    // Start workers to process results\n    const workers = Array(concurrencyLimit).fill(fileIterator).map(processFiles);\n    // Wait for all processing to complete.\n    await (0, Util_1.allSettledWithThrow)(workers);\n    return await this.createAndPoll(vectorStoreId, {\n      file_ids: allFileIds\n    });\n  }\n}\nexports.FileBatches = FileBatches;","map":{"version":3,"mappings":";;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;;;;AAEA;AAKA;AACA;AAEA;AAEA;AACA;AAEA,MAAaA,WAAY,SAAQC,sBAAW;EAC1C;;;EAGAC,MAAM,CACJC,aAAqB,EACrBC,IAA2B,EAC3BC,OAAwB;IAExB,OAAO,IAAI,CAACC,OAAO,CAACC,IAAI,EAAC,cAAI,uGAAkBJ,aAAa;MAC1DC;IAAI,GACDC,OAAO;MACVG,OAAO,EAAE,0BAAY,EAAC,CAAC;QAAE,aAAa,EAAE;MAAe,CAAE,EAAEH,OAAO,aAAPA,OAAO,uBAAPA,OAAO,CAAEG,OAAO,CAAC;IAAC,GAC7E;EACJ;EAEA;;;EAGAC,QAAQ,CACNC,OAAe,EACfC,MAA+B,EAC/BN,OAAwB;IAExB,MAAM;MAAEO;IAAe,CAAE,GAAGD,MAAM;IAClC,OAAO,IAAI,CAACL,OAAO,CAACO,GAAG,EAAC,cAAI,8GAAkBD,eAAe,EAAiBF,OAAO,mCAChFL,OAAO;MACVG,OAAO,EAAE,0BAAY,EAAC,CAAC;QAAE,aAAa,EAAE;MAAe,CAAE,EAAEH,OAAO,aAAPA,OAAO,uBAAPA,OAAO,CAAEG,OAAO,CAAC;IAAC,GAC7E;EACJ;EAEA;;;;EAIAM,MAAM,CACJJ,OAAe,EACfC,MAA6B,EAC7BN,OAAwB;IAExB,MAAM;MAAEO;IAAe,CAAE,GAAGD,MAAM;IAClC,OAAO,IAAI,CAACL,OAAO,CAACC,IAAI,EAAC,cAAI,qHAAkBK,eAAe,EAAiBF,OAAO,mCACjFL,OAAO;MACVG,OAAO,EAAE,0BAAY,EAAC,CAAC;QAAE,aAAa,EAAE;MAAe,CAAE,EAAEH,OAAO,aAAPA,OAAO,uBAAPA,OAAO,CAAEG,OAAO,CAAC;IAAC,GAC7E;EACJ;EAEA;;;EAGA,MAAMO,aAAa,CACjBC,aAAqB,EACrBZ,IAA2B,EAC3BC,OAAsD;IAEtD,MAAMY,KAAK,GAAG,MAAM,IAAI,CAACf,MAAM,CAACc,aAAa,EAAEZ,IAAI,CAAC;IACpD,OAAO,MAAM,IAAI,CAACc,IAAI,CAACF,aAAa,EAAEC,KAAK,CAACE,EAAE,EAAEd,OAAO,CAAC;EAC1D;EAEA;;;EAGAe,SAAS,CACPV,OAAe,EACfC,MAAgC,EAChCN,OAAwB;IAExB,MAAM;QAAEO;MAAyB,CAAE,GAAGD,MAAM;MAAhBU,KAAK,4BAAKV,MAAM;IAC5C,OAAO,IAAI,CAACL,OAAO,CAACgB,UAAU,EAC5B,cAAI,oHAAkBV,eAAe,EAAiBF,OAAO,GAC7Da,uBAAoC;MAClCF;IAAK,GAAKhB,OAAO;MAAEG,OAAO,EAAE,0BAAY,EAAC,CAAC;QAAE,aAAa,EAAE;MAAe,CAAE,EAAEH,OAAO,aAAPA,OAAO,uBAAPA,OAAO,CAAEG,OAAO,CAAC;IAAC,GACnG;EACH;EAEA;;;;;;EAMA,MAAMU,IAAI,CACRf,aAAqB,EACrBO,OAAe,EACfL,OAAsD;IAAA;IAEtD,MAAMG,OAAO,GAAG,0BAAY,EAAC,CAC3BH,OAAO,aAAPA,OAAO,uBAAPA,OAAO,CAAEG,OAAO,EAChB;MACE,yBAAyB,EAAE,MAAM;MACjC,kCAAkC,2BAAEH,OAAO,aAAPA,OAAO,iDAAPA,OAAO,CAAEmB,cAAc,2DAAvB,uBAAyBC,QAAQ,EAAE,yEAAIC;KAC5E,CACF,CAAC;IAEF,OAAO,IAAI,EAAE;MACX,MAAM;QAAEC,IAAI,EAAEV,KAAK;QAAEW;MAAQ,CAAE,GAAG,MAAM,IAAI,CAACnB,QAAQ,CACnDC,OAAO,EACP;QAAEE,eAAe,EAAET;MAAa,CAAE,kCAE7BE,OAAO;QACVG;MAAO,GAEV,CAACqB,YAAY,EAAE;MAEhB,QAAQZ,KAAK,CAACa,MAAM;QAClB,KAAK,aAAa;UAChB,IAAIC,aAAa,GAAG,IAAI;UAExB,IAAI1B,OAAO,aAAPA,OAAO,uBAAPA,OAAO,CAAEmB,cAAc,EAAE;YAC3BO,aAAa,GAAG1B,OAAO,CAACmB,cAAc;UACxC,CAAC,MAAM;YACL,MAAMQ,cAAc,GAAGJ,QAAQ,CAACpB,OAAO,CAACK,GAAG,CAAC,sBAAsB,CAAC;YACnE,IAAImB,cAAc,EAAE;cAClB,MAAMC,gBAAgB,GAAGC,QAAQ,CAACF,cAAc,CAAC;cACjD,IAAI,CAACG,KAAK,CAACF,gBAAgB,CAAC,EAAE;gBAC5BF,aAAa,GAAGE,gBAAgB;cAClC;YACF;UACF;UACA,MAAM,iBAAK,EAACF,aAAa,CAAC;UAC1B;QACF,KAAK,QAAQ;QACb,KAAK,WAAW;QAChB,KAAK,WAAW;UACd,OAAOd,KAAK;MAAC;IAEnB;EACF;EAEA;;;;;EAKA,MAAMmB,aAAa,CACjBpB,aAAqB,QAErBX,OAA+E;IAAA;IAAA,IAD/E;MAAEgC,KAAK;MAAEC,OAAO,GAAG;IAAE,CAA+C;IAGpE,IAAID,KAAK,IAAI,IAAI,IAAIA,KAAK,CAACE,MAAM,IAAI,CAAC,EAAE;MACtC,MAAM,IAAIC,KAAK,8GAEd;IACH;IAEA,MAAMC,qBAAqB,4BAAGpC,OAAO,aAAPA,OAAO,uBAAPA,OAAO,CAAEqC,cAAc,yEAAI,CAAC;IAE1D;IACA,MAAMC,gBAAgB,GAAGC,IAAI,CAACC,GAAG,CAACJ,qBAAqB,EAAEJ,KAAK,CAACE,MAAM,CAAC;IAEtE,MAAMO,MAAM,GAAG,IAAI,CAACxC,OAAO;IAC3B,MAAMyC,YAAY,GAAGV,KAAK,CAACW,MAAM,EAAE;IACnC,MAAMC,UAAU,GAAa,CAAC,GAAGX,OAAO,CAAC;IAEzC;IACA;IACA,eAAeY,YAAY,CAACC,QAAsC;MAChE,KAAK,IAAIC,IAAI,IAAID,QAAQ,EAAE;QACzB,MAAME,OAAO,GAAG,MAAMP,MAAM,CAACT,KAAK,CAACnC,MAAM,CAAC;UAAEoD,IAAI,EAAEF,IAAI;UAAEG,OAAO,EAAE;QAAY,CAAE,EAAElD,OAAO,CAAC;QACzF4C,UAAU,CAACO,IAAI,CAACH,OAAO,CAAClC,EAAE,CAAC;MAC7B;IACF;IAEA;IACA,MAAMsC,OAAO,GAAGC,KAAK,CAACf,gBAAgB,CAAC,CAACgB,IAAI,CAACZ,YAAY,CAAC,CAACa,GAAG,CAACV,YAAY,CAAC;IAE5E;IACA,MAAM,8BAAmB,EAACO,OAAO,CAAC;IAElC,OAAO,MAAM,IAAI,CAAC1C,aAAa,CAACC,aAAa,EAAE;MAC7C6C,QAAQ,EAAEZ;KACX,CAAC;EACJ;;AA5KFa","names":["FileBatches","resource_1","create","vectorStoreID","body","options","_client","post","headers","retrieve","batchID","params","vector_store_id","get","cancel","createAndPoll","vectorStoreId","batch","poll","id","listFiles","query","getAPIList","pagination_1","pollIntervalMs","toString","undefined","data","response","withResponse","status","sleepInterval","headerInterval","headerIntervalMs","parseInt","isNaN","uploadAndPoll","files","fileIds","length","Error","configuredConcurrency","maxConcurrency","concurrencyLimit","Math","min","client","fileIterator","values","allFileIds","processFiles","iterator","item","fileObj","file","purpose","push","workers","Array","fill","map","file_ids","exports"],"sourceRoot":"","sources":["../../src/resources/vector-stores/file-batches.ts"],"sourcesContent":[null]},"metadata":{},"sourceType":"script"}