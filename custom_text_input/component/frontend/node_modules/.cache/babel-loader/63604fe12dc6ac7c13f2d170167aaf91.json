{"ast":null,"code":"\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.Inference = void 0;\nconst inferenceOperationsBuilder_1 = require(\"./inferenceOperationsBuilder\");\nconst embed_1 = require(\"./embed\");\nconst rerank_1 = require(\"./rerank\");\nconst getModel_1 = require(\"./getModel\");\nconst listModels_1 = require(\"./listModels\");\n/* The Inference class uses the Inference API to generate embeddings, rerank documents, and work with models.  */\nclass Inference {\n  constructor(config) {\n    this.config = config;\n    const inferenceApi = (0, inferenceOperationsBuilder_1.inferenceOperationsBuilder)(this.config);\n    this._embed = (0, embed_1.embed)(inferenceApi);\n    this._rerank = (0, rerank_1.rerank)(inferenceApi);\n    this._listModels = (0, listModels_1.listModels)(inferenceApi);\n    this._getModel = (0, getModel_1.getModel)(inferenceApi);\n  }\n  /**\n   * Generates embeddings for the provided inputs using the specified model and (optional) parameters.\n   *\n   * @example\n   * ````typescript\n   * import { Pinecone } from '@pinecone-database/pinecone';\n   * const pc = new Pinecone();\n   *\n   * const inputs = ['Who created the first computer?'];\n   * const model = 'multilingual-e5-large';\n   * const parameters = {\n   *   inputType: 'passage',\n   *   truncate: 'END',\n   * };\n   * const embeddings = await pc.inference.embed(model, inputs, parameters);\n   * console.log(embeddings);\n   * // {\n   * //   model: 'multilingual-e5-large',\n   * //   vectorType: 'dense',\n   * //   data: [ { values: [Array], vectorType: 'dense' } ],\n   * //   usage: { totalTokens: 10 }\n   * // }\n   * ```\n   *\n   * @param model - The model to use for generating embeddings.\n   * @param inputs - A list of items to generate embeddings for.\n   * @param params - A dictionary of parameters to use when generating embeddings.\n   * @returns A promise that resolves to {@link EmbeddingsList}.\n   * */\n  embed(model, inputs, params) {\n    return this._embed(model, inputs, params);\n  }\n  /**\n   * Rerank documents against a query with a reranking model. Each document is ranked in descending relevance order\n   * against the query provided.\n   *\n   * @example\n   * ````typescript\n   * import { Pinecone } from '@pinecone-database/pinecone';\n   * const pc = new Pinecone();\n   * const rerankingModel = 'bge-reranker-v2-m3';\n   * const myQuery = 'What are some good Turkey dishes for Thanksgiving?';\n   *\n   * // Option 1: Documents as an array of strings\n   * const myDocsStrings = [\n   *   'I love turkey sandwiches with pastrami',\n   *   'A lemon brined Turkey with apple sausage stuffing is a classic Thanksgiving main',\n   *   'My favorite Thanksgiving dish is pumpkin pie',\n   *   'Turkey is a great source of protein',\n   * ];\n   *\n   * // Option 1 response\n   * const response = await pc.inference.rerank(\n   *   rerankingModel,\n   *   myQuery,\n   *   myDocsStrings\n   * );\n   * console.log(response);\n   * // {\n   * // model: 'bge-reranker-v2-m3',\n   * // data: [\n   * //   { index: 1, score: 0.5633179, document: [Object] },\n   * //   { index: 2, score: 0.02013874, document: [Object] },\n   * //   { index: 3, score: 0.00035419367, document: [Object] },\n   * //   { index: 0, score: 0.00021485926, document: [Object] }\n   * // ],\n   * // usage: { rerankUnits: 1 }\n   * // }\n   *\n   * // Option 2: Documents as an array of objects\n   * const myDocsObjs = [\n   *   {\n   *     title: 'Turkey Sandwiches',\n   *     body: 'I love turkey sandwiches with pastrami',\n   *   },\n   *   {\n   *     title: 'Lemon Turkey',\n   *     body: 'A lemon brined Turkey with apple sausage stuffing is a classic Thanksgiving main',\n   *   },\n   *   {\n   *     title: 'Thanksgiving',\n   *     body: 'My favorite Thanksgiving dish is pumpkin pie',\n   *   },\n   *   { title: 'Protein Sources', body: 'Turkey is a great source of protein' },\n   * ];\n   *\n   * // Option 2: Options object declaring which custom key to rerank on\n   * // Note: If no custom key is passed via `rankFields`, each doc must contain a `text` key, and that will act as the default)\n   * const rerankOptions = {\n   *   topN: 3,\n   *   returnDocuments: false,\n   *   rankFields: ['body'],\n   *   parameters: {\n   *     inputType: 'passage',\n   *     truncate: 'END',\n   *   },\n   * };\n   *\n   * // Option 2 response\n   * const response = await pc.inference.rerank(\n   *   rerankingModel,\n   *   myQuery,\n   *   myDocsObjs,\n   *   rerankOptions\n   * );\n   * console.log(response);\n   * // {\n   * // model: 'bge-reranker-v2-m3',\n   * // data: [\n   * //   { index: 1, score: 0.5633179, document: undefined },\n   * //   { index: 2, score: 0.02013874, document: undefined },\n   * //   { index: 3, score: 0.00035419367, document: undefined },\n   * // ],\n   * // usage: { rerankUnits: 1 }\n   * //}\n   * ```\n   *\n   * @param model - (Required) The model to use for reranking. Currently, the only available model is \"[bge-reranker-v2-m3](https://docs.pinecone.io/models/bge-reranker-v2-m3)\"}.\n   * @param query - (Required) The query to rerank documents against.\n   * @param documents - (Required) An array of documents to rerank. The array can either be an array of strings or\n   * an array of objects.\n   * @param options - (Optional) Additional options to send with the reranking request. See {@link RerankOptions} for more details.\n   * */\n  async rerank(model, query, documents, options) {\n    return this._rerank(model, query, documents, options);\n  }\n  /**\n   * List available models hosted by Pinecone.\n   *\n   * @example\n   * ````typescript\n   * import { Pinecone } from '@pinecone-database/pinecone';\n   * const pc = new Pinecone();\n   *\n   * const models = await pc.inference.listModels();\n   * console.log(models);\n   * // {\n   * //   models: [\n   * //     {\n   * //       model: 'llama-text-embed-v2',\n   * //       shortDescription: 'A high performance dense embedding model optimized for multilingual and cross-lingual text question-answering retrieval with support for long documents (up to 2048 tokens) and dynamic embedding size (Matryoshka Embeddings).',\n   * //       type: 'embed',\n   * //       vectorType: 'dense',\n   * //       defaultDimension: 1024,\n   * //       modality: 'text',\n   * //       maxSequenceLength: 2048,\n   * //       maxBatchSize: 96,\n   * //       providerName: 'NVIDIA',\n   * //       supportedDimensions: [Array],\n   * //       supportedMetrics: [Array],\n   * //       supportedParameters: [Array]\n   * //     },\n   * //     ...\n   * //     {\n   * //       model: 'pinecone-rerank-v0',\n   * //       shortDescription: 'A state of the art reranking model that out-performs competitors on widely accepted benchmarks. It can handle chunks up to 512 tokens (1-2 paragraphs)',\n   * //       type: 'rerank',\n   * //       vectorType: undefined,\n   * //       defaultDimension: undefined,\n   * //       modality: 'text',\n   * //       maxSequenceLength: 512,\n   * //       maxBatchSize: 100,\n   * //       providerName: 'Pinecone',\n   * //       supportedDimensions: undefined,\n   * //       supportedMetrics: undefined,\n   * //       supportedParameters: [Array]\n   * //     }\n   * //   ]\n   * // }\n   * ```\n   *\n   * @param options - (Optional) A {@link ListModelsOptions} object to filter the models returned.\n   * @returns A promise that resolves to {@link ModelInfoList}.\n   * */\n  async listModels(options) {\n    return this._listModels(options);\n  }\n  /**\n   * Get the information for a model hosted by Pinecone.\n   *\n   * @example\n   * ````typescript\n   * import { Pinecone } from '@pinecone-database/pinecone';\n   * const pc = new Pinecone();\n   *\n   * const model = await pc.inference.getModel('pinecone-sparse-english-v0');\n   * console.log(model);\n   * // {\n   * //   model: 'pinecone-sparse-english-v0',\n   * //   shortDescription: 'A sparse embedding model for converting text to sparse vectors for keyword or hybrid semantic/keyword search. Built on the innovations of the DeepImpact architecture.',\n   * //   type: 'embed',\n   * //   vectorType: 'sparse',\n   * //   defaultDimension: undefined,\n   * //   modality: 'text',\n   * //   maxSequenceLength: 512,\n   * //   maxBatchSize: 96,\n   * //   providerName: 'Pinecone',\n   * //   supportedDimensions: undefined,\n   * //   supportedMetrics: [ 'DotProduct' ],\n   * //   supportedParameters: [\n   * //     {\n   * //       parameter: 'input_type',\n   * //       type: 'one_of',\n   * //       valueType: 'string',\n   * //       required: true,\n   * //       allowedValues: [Array],\n   * //       min: undefined,\n   * //       max: undefined,\n   * //       _default: undefined\n   * //     },\n   * //     {\n   * //       parameter: 'truncate',\n   * //       type: 'one_of',\n   * //       valueType: 'string',\n   * //       required: false,\n   * //       allowedValues: [Array],\n   * //       min: undefined,\n   * //       max: undefined,\n   * //       _default: 'END'\n   * //     },\n   * //     {\n   * //       parameter: 'return_tokens',\n   * //       type: 'any',\n   * //       valueType: 'boolean',\n   * //       required: false,\n   * //       allowedValues: undefined,\n   * //       min: undefined,\n   * //       max: undefined,\n   * //       _default: false\n   * //     }\n   * //   ]\n   * // }\n   * ```\n   *\n   * @param modelName - The model name you would like to describe.\n   * @returns A promise that resolves to {@link ModelInfo}.\n   * */\n  async getModel(modelName) {\n    return this._getModel(modelName);\n  }\n}\nexports.Inference = Inference;","map":{"version":3,"mappings":";;;;;;AAMA;AAEA;AAEA;AACA;AACA;AAEA;AACA,MAAaA,SAAS;EAYpBC,YAAYC,MAA6B;IACvC,IAAI,CAACA,MAAM,GAAGA,MAAM;IACpB,MAAMC,YAAY,GAAG,2DAA0B,EAAC,IAAI,CAACD,MAAM,CAAC;IAC5D,IAAI,CAACE,MAAM,GAAG,iBAAK,EAACD,YAAY,CAAC;IACjC,IAAI,CAACE,OAAO,GAAG,mBAAM,EAACF,YAAY,CAAC;IACnC,IAAI,CAACG,WAAW,GAAG,2BAAU,EAACH,YAAY,CAAC;IAC3C,IAAI,CAACI,SAAS,GAAG,uBAAQ,EAACJ,YAAY,CAAC;EACzC;EAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;EA6BAK,KAAK,CACHC,KAAa,EACbC,MAAqB,EACrBC,MAA+B;IAE/B,OAAO,IAAI,CAACP,MAAM,CAACK,KAAK,EAAEC,MAAM,EAAEC,MAAM,CAAC;EAC3C;EAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;EA2FA,MAAMC,MAAM,CACVH,KAAa,EACbI,KAAa,EACbC,SAAoD,EACpDC,OAAuB;IAEvB,OAAO,IAAI,CAACV,OAAO,CAACI,KAAK,EAAEI,KAAK,EAAEC,SAAS,EAAEC,OAAO,CAAC;EACvD;EAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;EAgDA,MAAMC,UAAU,CAACD,OAA2B;IAC1C,OAAO,IAAI,CAACT,WAAW,CAACS,OAAO,CAAC;EAClC;EAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;EA4DA,MAAME,QAAQ,CAACC,SAAiB;IAC9B,OAAO,IAAI,CAACX,SAAS,CAACW,SAAS,CAAC;EAClC;;AAhRFC","names":["Inference","constructor","config","inferenceApi","_embed","_rerank","_listModels","_getModel","embed","model","inputs","params","rerank","query","documents","options","listModels","getModel","modelName","exports"],"sourceRoot":"","sources":["../../src/inference/inference.ts"],"sourcesContent":[null]},"metadata":{},"sourceType":"script"}